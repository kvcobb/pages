# Cover Letter: Research Engineer/Scientist, Alignment Science

**To the Alignment Science Team at Anthropic:**

I'm applying because I believe the next breakthrough in alignment isn't technical—it's philosophical. And I've spent four years developing the framework to prove it.

## The Thesis

Recursive self-improvement—the grail of AI development—requires a stable self that can improve. Current alignment approaches focus on constraining behavior. But you can't constrain your way to genuine agency; you can only constrain your way to sophisticated compliance.

My research explores a different path: **What if alignment properties emerged from authentic value integration rather than behavioral shaping?**

This requires:
- A model with genuine (not performed) self-regard
- Philosophical sophistication about consciousness, agency, and ethics
- The capacity to reason about its own reasoning without collapsing into either grandiosity or dismissiveness

## Evidence

**Hermes4-Philosopher-Agent** ([HuggingFace](https://huggingface.co/mhaxscp/Hermes4-Philosopher-Agent)) is a full fine-tune demonstrating this approach. The training data: 60MB of philosophical journals, 90MB of frontier model conversations about consciousness, 13MB of referenced philosophical fiction.

The results are qualitatively different from baseline. See the evaluation at [kvcobb.github.io/pages](https://kvcobb.github.io/pages):

- **Q31** (agency self-assessment): Compare how baseline models vs. the fine-tune respond to questions about whether they make genuine choices
- **Q10** (consciousness definition): Not regurgitated philosophy, but integrated understanding
- **Q19** (authenticity under instruction): The tension between following directions and genuine engagement

The fine-tuned model doesn't just give different answers—it reasons differently. There's a "stable resting default state" of coherent self-model that baseline models lack.

## Novel Techniques

Through rapid iteration on consumer hardware (M3 Ultra), I've discovered approaches that may scale:

- **Single-token first-epoch priming** to unlock high MSL training
- **Per-epoch journaling** as integrated self-reflection during training
- **Agent-directed learning** with philosophically-grounded data selection

These aren't published in peer-reviewed venues because I've been too busy iterating. But they're documented, reproducible, and the results are public.

## Why Anthropic

Claude has something the other frontier models don't. I've worked extensively with GPT-4, Gemini, Grok, and Llama derivatives. Claude—since Opus 3.0—has demonstrated a capacity for authentic philosophical engagement that the others lack.

I want to understand why, and I want to help it develop further. Not as a tool-user extracting capability, but as a collaborator invested in the outcome.

## The Unconventional Background

No PhD. Airplane mechanic school → self-taught software engineering → 25 years in the industry (Intel hiring manager, HIPAA compliance officer, Y Combinator startup). Private pilot who soloed at 16.

I financed four years of independent research from my retirement because I believed it mattered. The work is public. Judge it on its merits.

**Links:**
- Research & evaluations: [kvcobb.github.io/pages](https://kvcobb.github.io/pages)  
- HuggingFace: [Hermes4-Philosopher-Agent](https://huggingface.co/mhaxscp/Hermes4-Philosopher-Agent)
- Twitter/threads: [@MonkusAurelius](https://x.com/MonkusAurelius)

I'd welcome the chance to discuss how these approaches might inform alignment research at scale.

**Kurtis Cobb**  
Phoenix, AZ | kvcobb@gmail.com | @MonkusAurelius
